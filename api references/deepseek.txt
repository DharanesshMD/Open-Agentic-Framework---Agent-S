deepseek-ai / deepseek-r1-distill-qwen-32b
Model Overview
Background
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. DeepSeek-R1 sought to address these issues and further enhance reasoning performance by incorporating cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.

Description:

DeepSeek-Distill-Qwen-32B is distilled from DeepSeek-R1 based on Qwen2.5-32B. The reasoning patterns of larger models, DeepSeek-R1 in this case, can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. Using the reasoning data generated by DeepSeek-R1, dense models that are widely used in the research community can be fine-tuned. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.

This model is ready for commercial use.

Third-Party Community Consideration
This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to the DeepSeek-R1-Distill-Qwen-32B Model Card.

License/Terms of Use
The NIM container is governed by the NVIDIA Software License Agreement and Product-Specific Terms for AI Products; and the use of this model is governed by the NVIDIA Community Model License. ADDITIONAL INFORMATION: MIT License and Apache 2.0 License.

Model Developer: Deepseek-AI

Model Architecture
Architecture Type: Transformer
Network Architecture: Qwen
Version: 2.5

Input
Input Type: Text
Input Format: String
Input Parameters: 1D
Other Properties Related to Input:
DeepSeek recommends adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:

Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.
Avoid adding a system prompt; all instructions should be contained within the user prompt.
For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."
When evaluating model performance, it is recommended to conduct multiple tests and average the results.
Additionally, the DeepSeek-R1 series models tend to bypass thinking patterns (i.e., outputting "<think>\n\n</think>") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, DeepSeek recommends enforcing the model to initiate its response with "<think>\n" at the beginning of every output.

Output
Output Type: Text
Output Format: String
Output Parameters: 1D

Software Integration
Runtime Engine: TensorRT-LLM
Supported Hardware Microarchitecture Compatibility: NVIDIA Hopper, NVIDIA Lovelace
Preferred/Supported Operating System(s): Linux

Training, Testing, and Evaluation Dataset:
Training Dataset:
Data Collection Method by dataset: Automated
Labelling Method by dataset: Automated
Properties: 800k samples curated with DeepSeek-R1

Testing Dataset:
Data Collection Method by dataset: Automated. Reasoning data generated by DeepSeek-R1.
Labelling Method by dataset: Automated

Evaluation Dataset:
Link: See Evaluation section of the Hugging Face DeepSeek-R1-Distill-Qwen-32B Model Card
Data Collection Method by dataset: Hybrid: Human, Automated
Labeling Method by dataset: Hybrid: Human, Automated

Inference
Engine: TensorRT-LLM
Test Hardware: L20, H20

Ethical Considerations:
NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.

Please report security vulnerabilities or NVIDIA AI Concerns here.

Model Limitations: The base model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.

Creates a model response for the given chat conversation.
post
https://integrate.api.nvidia.com/v1/chat/completions
Given a list of messages comprising a conversation, the model will return a response. Compatible with OpenAI. See https://platform.openai.com/docs/api-reference/chat/create

Log in to see full request history
time	status	user agent	
Make a request to see history.
0 Requests This Month

Body Params
model
string
Defaults to deepseek-ai/deepseek-r1-distill-qwen-32b
deepseek-ai/deepseek-r1-distill-qwen-32b
messages
array of objects
required
A list of messages comprising the conversation so far. The roles of the messages must be alternating between user and assistant. The last input message should have role user. A message with the the system role is optional, and must be the very first message if it is present; context is also optional, but must come before a user question.


object

role
string
required
The role of the message author.


user
content
required
The contents of the message.


string

null

ADD object
temperature
number
0 to 1
Defaults to 0.6
The sampling temperature to use for text generation. The higher the temperature value is, the less deterministic the output text will be. It is not recommended to modify both temperature and top_p in the same call.

0.6
top_p
number
≤ 1
Defaults to 0.7
The top-p sampling mass used for text generation. The top-p value determines the probability mass that is sampled at sampling time. For example, if top_p = 0.2, only the most likely tokens (summing to 0.2 cumulative probability) will be sampled. It is not recommended to modify both temperature and top_p in the same call.

0.7
frequency_penalty
number
-2 to 2
Defaults to 0
Indicates how much to penalize new tokens based on their existing frequency in the text so far, decreasing model likelihood to repeat the same line verbatim.

0
presence_penalty
number
-2 to 2
Defaults to 0
Positive values penalize new tokens based on whether they appear in the text so far, increasing model likelihood to talk about new topics.

0
max_tokens
integer
1 to 4096
Defaults to 4096
The maximum number of tokens to generate in any given call. Note that the model is not aware of this value, and generation will simply stop at the number of tokens specified.

4096
stream
boolean
Defaults to false
If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events (SSE) as they become available (JSON responses are prefixed by data: ), with the stream terminated by a data: [DONE] message.


false
stop
A string or a list of strings where the API will stop generating further tokens. The returned text will not contain the stop sequence.


array

string

null
Responses

200
Invocation is fulfilled


202
Result is pending. Client should poll using the requestId.


422
Validation failed, provided entity could not be processed.


500
The invocation ended with an error.

import requests

url = "https://integrate.api.nvidia.com/v1/chat/completions"

payload = {
    "model": "deepseek-ai/deepseek-r1-distill-qwen-32b",
    "messages": [
        {
            "content": "I am going to Paris, what should I see?",
            "role": "user"
        }
    ],
    "temperature": 0.6,
    "top_p": 0.7,
    "frequency_penalty": 0,
    "presence_penalty": 0,
    "max_tokens": 4096,
    "stream": False,
    "stop": ["string"]
}
headers = {
    "accept": "application/json",
    "content-type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.text)